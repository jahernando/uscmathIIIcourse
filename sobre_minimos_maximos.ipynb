{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos III - sobre minímos y máximos\n",
    "\n",
    "\n",
    "**Course: \"Métodos III - Derivadas en varias dimensiones\"**\n",
    "\n",
    "*Author: Jose A. Hernando*, February 2018\n",
    "\n",
    "*Particle Physics Deparment. Universidade de Santiago de Compostela, Spain.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "#%matplotlib inline\n",
    "%matplotlib\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "# numpy and matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as optimize\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# possible styles: ggplot (simplicity), bmh (scientify data), \n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:** Generate an experiment that measured n-values of a quantity that is random distributed accordingly with a normal gaussian (mean zero and sigma one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values  [ 0.67287699  1.60581395 -0.97424339  0.59120874]\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "xs = stats.norm.rvs(size=n)\n",
    "fig, axs = plt.subplots(2, 1, figsize=(6, 6))\n",
    "axs[0].plot(xs, ls='none', marker='o')\n",
    "axs[0].set_xlabel('i-measurement')\n",
    "axs[1].set_ylabel('measurement')\n",
    "axs[1].hist(xs, 100);\n",
    "print('values ', xs[:4])\n",
    "axs[1].set_xlabel('value')\n",
    "axs[1].set_ylabel('frequency')\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "xs = stats.norm.rvs(size=n)\n",
    "\n",
    "def likelihood(xs, x):\n",
    "    pis = np.array([stats.norm.pdf(xi, x) for xi in xs])\n",
    "    return np.prod(pis)\n",
    "\n",
    "# loglikelihood\n",
    "def ll(x, xs):\n",
    "    dx = (x-xs)\n",
    "    return np.sum(dx*dx)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(xs, bins=50, alpha=0.5, range=(-3., 3.), color='blue')\n",
    "axb = ax.twinx()\n",
    "mus = np.linspace(-3., 3., 100)\n",
    "lls = np.array([ll(mu, xs) for mu in mus])\n",
    "axb.plot(mus, lls, alpha=0.5, color='red', lw=2)\n",
    "ax.set_xlabel('$x$', fontsize=16)\n",
    "ax.set_ylabel('samples'                        , color='blue', fontsize=14);\n",
    "axb.set_ylabel('$-2 \\, \\log \\, \\mathcal{L}(x)$', color='red' , fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squared fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FitResult = namedtuple('FitResult', 'params cov chi2')\n",
    "xsize = 100.\n",
    "\n",
    "def experiment(m, atrue=1, btrue=0, sigma=1.):\n",
    "    xdata = xsize*stats.uniform.rvs(size=m)\n",
    "    ydata = atrue*xdata + btrue + stats.norm.rvs(scale=sigma, size=m)\n",
    "    return (xdata, ydata)\n",
    "\n",
    "def straight_line(x, a, b):\n",
    "    return x*a + b\n",
    "\n",
    "def fit_experiment(xdata, ydata, func, npars=2):\n",
    "    popt, pcov = optimize.curve_fit(func, xdata, ydata)\n",
    "    ypred = func(xdata, *popt)\n",
    "    chi2 = np.sum((ydata-ypred)*(ydata-ypred)/(sigma*sigma))/(1.*(len(ydata)-len(popt)))\n",
    "    return FitResult(popt, pcov, chi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated parameters: a =  1.00953782095 , b=  -0.762303709705\n"
     ]
    }
   ],
   "source": [
    "atrue, btrue, sigma = 1., 0., 1.\n",
    "m = 10\n",
    "xdata, ydata = experiment(m, atrue, btrue, sigma)\n",
    "fitresult    = fit_experiment(xdata, ydata, straight_line)\n",
    "\n",
    "ahat, bhat = fitresult.params\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "print('estimated parameters:', 'a = ', ahat, ', b= ',bhat)\n",
    "\n",
    "# plot data\n",
    "ax.plot(xdata, ydata             , color='black', marker='o', markersize=5, ls='none', label='data')\n",
    "ax.plot(xdata,  ahat*xdata + bhat, color='blue' , lw=2, label='ls fit', alpha=0.5)\n",
    "ax.plot(xdata, atrue*xdata + btrue, color='green', lw=2,  label='true'  , alpha=0.5)\n",
    "ax.legend(loc=2, fontsize=14)\n",
    "ax.set_xlabel(r'$x$', fontsize=14)\n",
    "ax.set_ylabel(r'$y$', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Generate n-large experiments with m-samples and obtaine the distribution of the $\\chi^2/ndf$, the parameters and the pulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a pull: mu =  0.00655757694981 sigma =  3.11781209927\n",
      "b pull: mu =  -0.0146856501754 sigma =  3.10836521071\n",
      "chi2: mean  0.996337590693\n"
     ]
    }
   ],
   "source": [
    "n = 10000   # number of experiments\n",
    "m = 4     # number of points per experiment\n",
    "atrue, btrue = 1, 0\n",
    "nbins = 40\n",
    "\n",
    "datas      = [experiment(m, atrue, btrue) for i in range(n)]\n",
    "fitresults = [fit_experiment(xdata, ydata, straight_line) for xdata, ydata in datas]\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(9, 12))\n",
    "\n",
    "# parameters\n",
    "ahats = [fitresult.params[0] for fitresult in fitresults]\n",
    "axs[0, 0].hist(ahats, nbins)\n",
    "axs[0, 0].set_xlabel('$\\hat{a}$')\n",
    "bhats = [fitresult.params[1] for fitresult in fitresults]\n",
    "axs[0, 1].hist(bhats, nbins)\n",
    "axs[0, 1].set_xlabel('$\\hat{b}$')\n",
    "\n",
    "# errors\n",
    "perrors = [np.sqrt(np.diag(fitresult.cov)) for fitresult in fitresults]\n",
    "asigmas = [perror[0] for perror in perrors]\n",
    "bsigmas = [perror[1] for perror in perrors]\n",
    "axs[1, 0].hist(asigmas, nbins)\n",
    "axs[1, 0].set_xlabel('$\\sigma_a$')\n",
    "axs[1, 1].hist(bsigmas, nbins)\n",
    "axs[1, 1].set_xlabel('$\\sigma_b$')\n",
    "\n",
    "# pulls\n",
    "apulls = (np.array(ahats)-atrue)/np.array(asigmas)\n",
    "bpulls = (np.array(bhats)-btrue)/np.array(bsigmas)\n",
    "print('a pull: mu = ', np.mean(apulls), 'sigma = ', np.std(apulls))\n",
    "print('b pull: mu = ', np.mean(bpulls), 'sigma = ', np.std(bpulls))\n",
    "axs[2, 0].hist(apulls, nbins)\n",
    "axs[2, 0].set_xlabel('$(\\hat{a}-a_{True})/\\sigma_a$')\n",
    "axs[2, 1].hist(bpulls, nbins)\n",
    "axs[2, 1].set_xlabel('$(\\hat{b}-b_{True})/\\sigma_b$')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "chi2 = [fitresult.chi2 for fitresult in fitresults]\n",
    "print('chi2: mean ', np.mean(np.array(chi2)))\n",
    "ax.hist(chi2, nbins)\n",
    "ax.set_xlabel('$\\chi^2/ndf$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Neural Networks (NN) is a web of nodes (neurons) organized in layers. Each neuron gets several inputs (the initial values or the output or other neurons) and response with an output usually in the $[0, 1]$ range. The last layer has a neuron that provides the target decision. NN mimics the human nervous system. \n",
    "\n",
    "A neuron response is a faction of a linear combination of its $n$-inputs ($x_i$). It has $n+1$ parameters:\n",
    "\n",
    "$$\n",
    "u(a_0 + \\sum_{i=1}^n a_i \\, x_i)\n",
    "$$\n",
    "\n",
    "The function $u(x)$ is usually a sigmoid:\n",
    "$$\n",
    "u(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "NN are trained via down-back modification of neurons parameters via minimizing an error  function for the last neuron:\n",
    "\n",
    "$$\n",
    "E(x, y) = \\sum_i (u(x_i)-y_i)^2\n",
    "$$\n",
    "\n",
    "where $x_i$ are the samples and $y_i$ the target. \n",
    "\n",
    "Recently, new computational techniques, has boosted NN, allowing to have a greather number of layers and a faster learning rate. These NN are called Deep Neural Network, for deep learning. This revolution was started by the Google company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-1., 1., 100)\n",
    "w =  100.\n",
    "b =  0.\n",
    "ys = 1./(1.+np.exp(-w*xs-b))\n",
    "plt.plot(xs, ys);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "X_train = data[:n_samples//2]\n",
    "y_train = digits.target[:n_samples//2]\n",
    "\n",
    "\n",
    "def plot_digit(index, show=True):\n",
    "    image, target = digits.images[index], digits.target[index]\n",
    "    plt.subplot()\n",
    "    plt.imshow(image, cmap=plt.cm.inferno, interpolation='nearest')\n",
    "    if (show):\n",
    "        plt.title('true: %i' % target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 163\n",
    "show = 0\n",
    "plot_digit(index, show = show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "n_train = n_samples // 2 \n",
    "\n",
    "X_train = data         [        : n_train]\n",
    "y_train = digits.target[        : n_train]\n",
    "\n",
    "X_test  = data         [n_train :        ]\n",
    "y_test  = digits.target[n_train :        ]\n",
    "\n",
    "\n",
    "classifier = MLPClassifier(solver='adam')\n",
    "\n",
    "# We learn the digits on the first half of the digits\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Now predict the value of the digit on the second half:\n",
    "y_predicted = classifier.predict(X_test)\n",
    "#plt.hist(y_predicted, 100);\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.grid(False)\n",
    "_, _, _, ct = ax.hist2d(y_test, y_predicted, cmap=plt.cm.jet);\n",
    "ax.set_xlabel('y-test')\n",
    "ax.set_ylabel('y-predicted');\n",
    "plt.colorbar(ct, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Predicted : 9 , True :  9\n"
     ]
    }
   ],
   "source": [
    "oks = [yp == yt for yp, yt in zip(y_predicted, y_test)]\n",
    "#print(oks)\n",
    "index = 6\n",
    "show  = 1\n",
    "if (show):\n",
    "    print(' Predicted :', y_predicted[index], ', True : ', y_test[index])\n",
    "plot_digit(n_train+index, show = show)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-06 10:37:52,894 Downloading LFW data (~200MB): http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-06 10:38:52,583 Decompressing the data archive to /Users/hernando/scikit_learn_data/lfw_home/lfw_funneled\n",
      "2018-03-06 10:39:06,403 Loading LFW people faces from /Users/hernando/scikit_learn_data/lfw_home\n",
      "2018-03-06 10:39:06,823 Loading face #00001 / 01288\n",
      "2018-03-06 10:39:11,205 Loading face #01001 / 01288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size:\n",
      "n_samples: 1288\n",
      "n_features: 1850\n",
      "n_classes: 7\n",
      "Extracting the top 150 eigenfaces from 966 faces\n",
      "done in 0.480s\n",
      "Projecting the input data on the eigenfaces orthonormal basis\n",
      "done in 0.034s\n",
      "Fitting the classifier to the training set\n",
      "done in 24.261s\n",
      "Best estimator found by grid search:\n",
      "SVC(C=1000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Predicting people's names on the test set\n",
      "done in 0.054s\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "     Ariel Sharon       0.57      0.62      0.59        13\n",
      "     Colin Powell       0.74      0.88      0.80        60\n",
      "  Donald Rumsfeld       0.73      0.70      0.72        27\n",
      "    George W Bush       0.90      0.88      0.89       146\n",
      "Gerhard Schroeder       0.83      0.76      0.79        25\n",
      "      Hugo Chavez       0.73      0.53      0.62        15\n",
      "       Tony Blair       0.84      0.75      0.79        36\n",
      "\n",
      "      avg / total       0.82      0.82      0.82       322\n",
      "\n",
      "[[  8   1   2   1   0   0   1]\n",
      " [  1  53   2   2   0   1   1]\n",
      " [  5   1  19   1   0   1   0]\n",
      " [  0  11   2 129   2   0   2]\n",
      " [  0   1   0   3  19   1   1]\n",
      " [  0   2   0   3   2   8   0]\n",
      " [  0   3   1   5   0   0  27]]\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n",
    "# dataset): unsupervised feature extraction / dimensionality reduction\n",
    "n_components = 150\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "t0 = time()\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "          whiten=True).fit(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "t0 = time()\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Train a SVM classification model\n",
    "\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "t0 = time()\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
    "clf = clf.fit(X_train_pca, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Quantitative evaluation of the model quality on the test set\n",
    "\n",
    "print(\"Predicting people's names on the test set\")\n",
    "t0 = time()\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Qualitative evaluation of the predictions using matplotlib\n",
    "\n",
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "\n",
    "# plot the result of the prediction on a portion of the test set\n",
    "\n",
    "def title(y_pred, y_test, target_names, i):\n",
    "    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n",
    "    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n",
    "    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)\n",
    "\n",
    "prediction_titles = [title(y_pred, y_test, target_names, i)\n",
    "                     for i in range(y_pred.shape[0])]\n",
    "\n",
    "plot_gallery(X_test, prediction_titles, h, w)\n",
    "\n",
    "# plot the gallery of the most significative eigenfaces\n",
    "\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
